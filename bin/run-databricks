#!/usr/bin/env python3

import argparse
import os
import json
import logging
from base64 import b64encode
from textwrap import dedent

from urllib import request, parse


def api_request(instance, route, data, token):
    api_endpoint = f"https://{instance}/{route.lstrip('/')}"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    data = json.dumps(data).encode()
    req = request.Request(api_endpoint, data=data, headers=headers)
    resp = request.urlopen(req)
    logging.info(f"status: {resp.getcode()} info: {resp.info()}")
    return resp


def generate_runner(module_name, instance, token):
    """Generate a runner for the current module to be run in Databricks."""

    runner_data = f"""
    # This runner has been auto-generated from mozilla/python_mozetl/bin/mozetl-databricks.py.
    # Any changes made to the runner file will be over-written on subsequent runs.
    from {module_name} import cli

    try:
        cli.entry_point(auto_envvar_prefix="MOZETL")
    except SystemExit:
        # avoid calling sys.exit() in databricks
        # http://click.palletsprojects.com/en/7.x/api/?highlight=auto_envvar_prefix#click.BaseCommand.main
        pass
    """
    logging.debug(dedent(runner_data))

    data = {
        "contents": b64encode(dedent(runner_data).encode()).decode(),
        "overwrite": True,
        "path": f"/FileStore/airflow/{module_name}_runner.py",
    }
    logging.debug(json.dumps(data, indent=2))
    resp = api_request(instance, "/api/2.0/dbfs/put", data, token)
    logging.info(resp.read())


def run_submit(args):
    config = {
        "run_name": "mozetl local submission",
        "new_cluster": {
            "spark_version": "6.1.x-scala2.11",
            "node_type_id": "c3.4xlarge",
            "num_workers": args.num_workers,
            "aws_attributes": {
                "availability": "ON_DEMAND",
                "instance_profile_arn": "arn:aws:iam::144996185633:instance-profile/databricks-ec2",
            },
        },
        "spark_python_task": {
            "python_file": f"dbfs:/FileStore/airflow/{args.module_name}_runner.py",
            "parameters": args.command,
        },
        "libraries": {"pypi": {"package": f"git+{args.git_path}@{args.git_branch}"}},
    }

    logging.debug(json.dumps(config, indent=2))

    # https://docs.databricks.com/api/latest/jobs.html#runs-submit
    resp = api_request(args.instance, "/api/2.0/jobs/runs/submit", config, args.token)
    logging.info(resp.read())


def parse_arguments():
    parser = argparse.ArgumentParser(description="run bgbb")
    parser.add_argument(
        "--git-path",
        type=str,
        default="https://github.com/wcbeard/bgbb_airflow.git",
        help="The URL to the git repository e.g. https://github.com/wcbeard/bgbb_airflow.git",
    )
    parser.add_argument(
        "--git-branch", type=str, default="master", help="The branch to run e.g. master"
    )
    parser.add_argument(
        "--num-workers",
        type=int,
        default=2,
        help="Number of worker instances to spawn in the cluster",
    )
    parser.add_argument(
        "--token",
        type=str,
        required=True,
        help="A Databricks authorization token, generated from the user settings page",
    )
    parser.add_argument(
        "--instance",
        type=str,
        default="dbc-caf9527b-e073.cloud.databricks.com",
        help="The Databricks instance.",
    )
    parser.add_argument(
        "--module-name",
        type=str,
        default="bgbb_airflow",
        help="Top-level module name to run",
    )
    parser.add_argument(
        "command", nargs=argparse.REMAINDER, help="Arguments to pass to bgbb_airflow"
    )
    args = parser.parse_args()
    return args


if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG)

    args = parse_arguments()
    generate_runner(args.module_name, args.instance, args.token)
    run_submit(args)
